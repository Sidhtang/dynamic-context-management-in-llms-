{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOmHyU/Vfji8z4Hf6tedmVP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sidhtang/dynamic-context-management-in-llms-/blob/main/adaptive_sliding_window_adaptation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ql1Rgiijh6tq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "T1I98qpeh63w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import deque\n",
        "from typing import List, Dict, Any, Optional, Set\n",
        "from dataclasses import dataclass\n",
        "from datetime import datetime\n",
        "import numpy as np\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModel,\n",
        "    pipeline,\n",
        "    T5ForConditionalGeneration,\n",
        "    T5Tokenizer\n",
        ")\n",
        "import torch\n",
        "import spacy\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "@dataclass\n",
        "class ContextChunk:\n",
        "    \"\"\"Represents a chunk of context with metadata\"\"\"\n",
        "    text: str\n",
        "    timestamp: datetime\n",
        "    importance_score: float\n",
        "    embedding: Optional[np.ndarray] = None\n",
        "    summary: Optional[str] = None\n",
        "    entities: Optional[List[Dict]] = None\n",
        "    keywords: Optional[Set[str]] = None\n",
        "\n",
        "class EnhancedNLPProcessor:\n",
        "    def __init__(self):\n",
        "        # Initialize SpaCy pipeline with custom settings\n",
        "        self.nlp = spacy.load('en_core_web_sm')\n",
        "        # Add custom entity patterns\n",
        "        ruler = self.nlp.get_pipe(\"entity_ruler\") if \"entity_ruler\" in self.nlp.pipe_names else self.nlp.add_pipe(\"entity_ruler\")\n",
        "        patterns = [\n",
        "            {\"label\": \"ORG\", \"pattern\": \"OpenAI\"},\n",
        "            {\"label\": \"ORG\", \"pattern\": \"DeepMind\"},\n",
        "            {\"label\": \"TECH\", \"pattern\": \"AI model\"},\n",
        "            {\"label\": \"TECH\", \"pattern\": \"software library\"}\n",
        "        ]\n",
        "        ruler.add_patterns(patterns)\n",
        "\n",
        "        # Initialize transformers\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
        "        self.summarizer = T5ForConditionalGeneration.from_pretrained('t5-small').to(self.device)\n",
        "        self.sentence_transformer = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "    def extract_entities(self, text: str) -> List[Dict]:\n",
        "        \"\"\"Extract named entities with improved handling\"\"\"\n",
        "        doc = self.nlp(text)\n",
        "        entities = []\n",
        "        seen = set()  # To prevent duplicate entities\n",
        "\n",
        "        for ent in doc.ents:\n",
        "            # Normalize entity text\n",
        "            normalized_text = ent.text.strip()\n",
        "\n",
        "            # Skip if we've seen this entity before\n",
        "            if (normalized_text, ent.label_) in seen:\n",
        "                continue\n",
        "\n",
        "            # Add entity with additional context\n",
        "            entities.append({\n",
        "                'text': normalized_text,\n",
        "                'label': ent.label_,\n",
        "                'start_char': ent.start_char,\n",
        "                'end_char': ent.end_char,\n",
        "                'context': text[max(0, ent.start_char-20):min(len(text), ent.end_char+20)]\n",
        "            })\n",
        "\n",
        "            seen.add((normalized_text, ent.label_))\n",
        "\n",
        "        return entities\n",
        "\n",
        "    def generate_summary(self, text: str, max_length: int = 50) -> str:\n",
        "        \"\"\"Generate improved summary with better formatting\"\"\"\n",
        "        # Add explicit summarization prompt\n",
        "        prompt = \"summarize concisely: \" + text\n",
        "\n",
        "        inputs = self.tokenizer.encode(prompt,\n",
        "                                     return_tensors=\"pt\",\n",
        "                                     max_length=512,\n",
        "                                     truncation=True).to(self.device)\n",
        "\n",
        "        summary_ids = self.summarizer.generate(\n",
        "            inputs,\n",
        "            max_length=max_length,\n",
        "            min_length=10,\n",
        "            num_beams=4,\n",
        "            length_penalty=2.0,  # Encourage slightly longer summaries\n",
        "            no_repeat_ngram_size=2,  # Avoid repetition\n",
        "            early_stopping=True\n",
        "        )\n",
        "\n",
        "        summary = self.tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "        # Post-process summary\n",
        "        summary = summary.strip()\n",
        "        if not summary.endswith('.'):\n",
        "            summary += '.'\n",
        "\n",
        "        # Ensure first letter is capitalized\n",
        "        summary = summary[0].upper() + summary[1:]\n",
        "\n",
        "        return summary\n",
        "\n",
        "class EnhancedAdaptiveSlidingWindow:\n",
        "    def __init__(self, max_chunks: int = 5, similarity_threshold: float = 0.7, importance_threshold: float = 0.3):\n",
        "        \"\"\"\n",
        "        Initialize the Enhanced Adaptive Sliding Window.\n",
        "\n",
        "        Args:\n",
        "            max_chunks (int): Maximum number of context chunks to maintain\n",
        "            similarity_threshold (float): Threshold for determining similar content\n",
        "            importance_threshold (float): Minimum importance score for retention\n",
        "        \"\"\"\n",
        "        self.max_chunks = max_chunks\n",
        "        self.similarity_threshold = similarity_threshold\n",
        "        self.importance_threshold = importance_threshold\n",
        "        self.context_chunks = deque(maxlen=max_chunks)\n",
        "        self.nlp_processor = EnhancedNLPProcessor()\n",
        "\n",
        "    def add_chunk(self, text: str) -> None:\n",
        "        \"\"\"Add a new chunk of text to the context window\"\"\"\n",
        "        # Create embedding\n",
        "        embedding = self.nlp_processor.sentence_transformer.encode([text])[0]\n",
        "\n",
        "        # Calculate importance score (simplified version)\n",
        "        importance_score = len(text.split()) / 100  # Basic score based on length\n",
        "\n",
        "        # Extract entities\n",
        "        entities = self.nlp_processor.extract_entities(text)\n",
        "\n",
        "        # Generate summary\n",
        "        summary = self.nlp_processor.generate_summary(text)\n",
        "\n",
        "        # Create new chunk\n",
        "        chunk = ContextChunk(\n",
        "            text=text,\n",
        "            timestamp=datetime.now(),\n",
        "            importance_score=importance_score,\n",
        "            embedding=embedding,\n",
        "            summary=summary,\n",
        "            entities=entities,\n",
        "            keywords=set(word.lower() for word in text.split())\n",
        "        )\n",
        "\n",
        "        # Check similarity with existing chunks\n",
        "        if self.context_chunks:\n",
        "            similarities = [\n",
        "                cosine_similarity(\n",
        "                    [chunk.embedding],\n",
        "                    [existing_chunk.embedding]\n",
        "                )[0][0]\n",
        "                for existing_chunk in self.context_chunks\n",
        "            ]\n",
        "\n",
        "            # If too similar to existing content, update importance scores\n",
        "            if max(similarities) > self.similarity_threshold:\n",
        "                return\n",
        "\n",
        "        # Add new chunk\n",
        "        self.context_chunks.append(chunk)\n",
        "\n",
        "        # Remove least important chunks if over capacity\n",
        "        while len(self.context_chunks) > self.max_chunks:\n",
        "            min_importance = min(c.importance_score for c in self.context_chunks)\n",
        "            if min_importance < self.importance_threshold:\n",
        "                self.context_chunks.remove(min(\n",
        "                    self.context_chunks,\n",
        "                    key=lambda x: x.importance_score\n",
        "                ))\n",
        "\n",
        "    def get_important_entities(self) -> Dict[str, List[Dict]]:\n",
        "        \"\"\"Get all named entities with improved organization and context\"\"\"\n",
        "        entities_by_type = {}\n",
        "\n",
        "        for chunk in self.context_chunks:\n",
        "            if chunk.entities:\n",
        "                for entity in chunk.entities:\n",
        "                    if entity['label'] not in entities_by_type:\n",
        "                        entities_by_type[entity['label']] = []\n",
        "\n",
        "                    # Add entity with frequency and context\n",
        "                    existing = next((e for e in entities_by_type[entity['label']]\n",
        "                                   if e['text'].lower() == entity['text'].lower()), None)\n",
        "\n",
        "                    if existing:\n",
        "                        existing['frequency'] += 1\n",
        "                        if entity['context'] not in existing['contexts']:\n",
        "                            existing['contexts'].append(entity['context'])\n",
        "                    else:\n",
        "                        entities_by_type[entity['label']].append({\n",
        "                            'text': entity['text'],\n",
        "                            'frequency': 1,\n",
        "                            'contexts': [entity['context']]\n",
        "                        })\n",
        "\n",
        "        # Sort entities by frequency within each type\n",
        "        for entity_type in entities_by_type:\n",
        "            entities_by_type[entity_type].sort(key=lambda x: x['frequency'], reverse=True)\n",
        "\n",
        "        return entities_by_type\n",
        "\n",
        "    def get_context_summary(self) -> str:\n",
        "        \"\"\"Get improved context summary\"\"\"\n",
        "        if not self.context_chunks:\n",
        "            return \"\"\n",
        "\n",
        "        # Collect all summaries with their importance scores\n",
        "        summaries = [(chunk.summary, chunk.importance_score)\n",
        "                    for chunk in self.context_chunks if chunk.summary]\n",
        "\n",
        "        # Sort by importance score\n",
        "        summaries.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        # Combine summaries, giving more weight to important ones\n",
        "        combined_text = \" \".join(summary for summary, _ in summaries)\n",
        "\n",
        "        # Generate final summary\n",
        "        return self.nlp_processor.generate_summary(combined_text)\n",
        "\n",
        "def main():\n",
        "    window = EnhancedAdaptiveSlidingWindow(\n",
        "        max_chunks=5,\n",
        "        similarity_threshold=0.7,\n",
        "        importance_threshold=0.3\n",
        "    )\n",
        "\n",
        "    texts = [\n",
        "        \"\"\"The new AI model has achieved breakthrough performance on multiple\n",
        "        benchmarks. Researchers at OpenAI and DeepMind contributed to this\n",
        "        development.\"\"\",\n",
        "\n",
        "        \"\"\"Weather conditions remain stable with mild temperatures and clear\n",
        "        skies.\"\"\",\n",
        "\n",
        "        \"\"\"Critical security vulnerability discovered in widely-used software\n",
        "        library. Users urged to update immediately.\"\"\",\n",
        "\n",
        "        \"\"\"Project team meeting scheduled for tomorrow at 2 PM to discuss Q4\n",
        "        deliverables and strategy.\"\"\"\n",
        "    ]\n",
        "\n",
        "    for text in texts:\n",
        "        print(\"\\nProcessing new text:\", text)\n",
        "        window.add_chunk(text)\n",
        "\n",
        "        print(\"\\nCurrent context summary:\")\n",
        "        print(window.get_context_summary())\n",
        "\n",
        "        print(\"\\nImportant entities:\")\n",
        "        entities = window.get_important_entities()\n",
        "        for entity_type, entity_list in entities.items():\n",
        "            print(f\"\\n{entity_type}:\")\n",
        "            for entity in entity_list:\n",
        "                print(f\"- {entity['text']} (mentioned {entity['frequency']} times)\")\n",
        "                for context in entity['contexts']:\n",
        "                    print(f\"  Context: ...{context}...\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CT-c_xjWh6_0",
        "outputId": "4a3616ec-7e2f-4e69-cd9d-2d262f4dd9c1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Processing new text: The new AI model has achieved breakthrough performance on multiple \n",
            "        benchmarks. Researchers at OpenAI and DeepMind contributed to this \n",
            "        development.\n",
            "\n",
            "Current context summary:\n",
            "The new AI model has achieved breakthrough performance on multiple benchmarks. researchers at OpenAI and DeepMind contributed to this development.\n",
            "\n",
            "Important entities:\n",
            "\n",
            "ORG:\n",
            "- AI (mentioned 1 times)\n",
            "  Context: ...The new AI model has achieved ...\n",
            "\n",
            "GPE:\n",
            "- OpenAI (mentioned 1 times)\n",
            "  Context: ...rks. Researchers at OpenAI and DeepMind contri...\n",
            "\n",
            "PRODUCT:\n",
            "- DeepMind (mentioned 1 times)\n",
            "  Context: ...chers at OpenAI and DeepMind contributed to this...\n",
            "\n",
            "Processing new text: Weather conditions remain stable with mild temperatures and clear \n",
            "        skies.\n",
            "\n",
            "Current context summary:\n",
            "New AI model has achieved breakthrough performance on multiple benchmarks. weather conditions remain stable with mild temperatures and clear skies.\n",
            "\n",
            "Important entities:\n",
            "\n",
            "ORG:\n",
            "- AI (mentioned 1 times)\n",
            "  Context: ...The new AI model has achieved ...\n",
            "\n",
            "GPE:\n",
            "- OpenAI (mentioned 1 times)\n",
            "  Context: ...rks. Researchers at OpenAI and DeepMind contri...\n",
            "\n",
            "PRODUCT:\n",
            "- DeepMind (mentioned 1 times)\n",
            "  Context: ...chers at OpenAI and DeepMind contributed to this...\n",
            "\n",
            "Processing new text: Critical security vulnerability discovered in widely-used software \n",
            "        library. Users urged to update immediately.\n",
            "\n",
            "Current context summary:\n",
            "Critical security vulnerability discovered in widely-used software library. weather conditions remain stable with mild temperatures and clear skies.\n",
            "\n",
            "Important entities:\n",
            "\n",
            "ORG:\n",
            "- AI (mentioned 1 times)\n",
            "  Context: ...The new AI model has achieved ...\n",
            "\n",
            "GPE:\n",
            "- OpenAI (mentioned 1 times)\n",
            "  Context: ...rks. Researchers at OpenAI and DeepMind contri...\n",
            "\n",
            "PRODUCT:\n",
            "- DeepMind (mentioned 1 times)\n",
            "  Context: ...chers at OpenAI and DeepMind contributed to this...\n",
            "\n",
            "Processing new text: Project team meeting scheduled for tomorrow at 2 PM to discuss Q4 \n",
            "        deliverables and strategy.\n",
            "\n",
            "Current context summary:\n",
            "Researchers at openAI and DeepMind contributed to this development. critical security vulnerability discovered in widely-used software library. users urged to update immediately.\n",
            "\n",
            "Important entities:\n",
            "\n",
            "ORG:\n",
            "- AI (mentioned 1 times)\n",
            "  Context: ...The new AI model has achieved ...\n",
            "\n",
            "GPE:\n",
            "- OpenAI (mentioned 1 times)\n",
            "  Context: ...rks. Researchers at OpenAI and DeepMind contri...\n",
            "\n",
            "PRODUCT:\n",
            "- DeepMind (mentioned 1 times)\n",
            "  Context: ...chers at OpenAI and DeepMind contributed to this...\n",
            "\n",
            "DATE:\n",
            "- tomorrow (mentioned 1 times)\n",
            "  Context: ...eting scheduled for tomorrow at 2 PM to discuss ...\n",
            "\n",
            "TIME:\n",
            "- 2 PM (mentioned 1 times)\n",
            "  Context: ...led for tomorrow at 2 PM to discuss Q4 \n",
            "    ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ball1TNYlGeT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0YW9ds_8lGhA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}